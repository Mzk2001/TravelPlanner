import { useState, useRef, useCallback } from 'react';

const useVoiceRecognition = () => {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [error, setError] = useState(null);
  const [transcript, setTranscript] = useState('');
  const [interimTranscript, setInterimTranscript] = useState('');
  
  const recognitionRef = useRef(null);

  const startRecording = useCallback(async () => {
    try {
      console.log('ðŸŽ¤ å¼€å§‹è¯­éŸ³è¯†åˆ«...');
      setError(null);
      setTranscript('');
      setInterimTranscript('');

      // æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦æ”¯æŒWeb Speech API
      if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
        console.error('âŒ æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«');
        throw new Error('æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ï¼Œè¯·ä½¿ç”¨Chromeã€Edgeæˆ–Safariæµè§ˆå™¨');
      }

      console.log('âœ… æµè§ˆå™¨æ”¯æŒè¯­éŸ³è¯†åˆ«');

      // åˆ›å»ºè¯­éŸ³è¯†åˆ«å®žä¾‹
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      const recognition = new SpeechRecognition();
      
      recognitionRef.current = recognition;

      // é…ç½®è¯­éŸ³è¯†åˆ«å‚æ•°
      recognition.continuous = true; // è¿žç»­è¯†åˆ«ï¼Œç”¨æˆ·å¯ä»¥æ‰‹åŠ¨åœæ­¢
      recognition.interimResults = true; // è¿”å›žä¸­é—´ç»“æžœï¼Œå®žæ—¶æ˜¾ç¤ºè¯†åˆ«å†…å®¹
      recognition.lang = 'zh-CN'; // ä¸­æ–‡è¯†åˆ«
      recognition.maxAlternatives = 1; // åªè¿”å›žæœ€ä½³ç»“æžœ

      console.log('ðŸ”§ è¯­éŸ³è¯†åˆ«é…ç½®å®Œæˆ');

      // è¯†åˆ«ç»“æžœå¤„ç†
      recognition.onresult = (event) => {
        console.log('ðŸŽ¯ è¯­éŸ³è¯†åˆ«ç»“æžœ:', event);
        let finalTranscript = '';
        let interimTranscript = '';

        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript;
          if (event.results[i].isFinal) {
            finalTranscript += transcript;
          } else {
            interimTranscript += transcript;
          }
        }

        console.log('ðŸ“ æœ€ç»ˆç»“æžœ:', finalTranscript);
        console.log('â³ ä¸­é—´ç»“æžœ:', interimTranscript);

        if (finalTranscript) {
          setTranscript(prev => prev + finalTranscript);
        }
        if (interimTranscript) {
          setInterimTranscript(interimTranscript);
        }
      };

      // è¯†åˆ«ç»“æŸå¤„ç†
      recognition.onend = () => {
        console.log('ðŸ è¯­éŸ³è¯†åˆ«ç»“æŸ');
        setIsRecording(false);
        setIsProcessing(false);
        setInterimTranscript('');
      };

      // è¯†åˆ«é”™è¯¯å¤„ç†
      recognition.onerror = (event) => {
        console.error('âŒ è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error);
        let errorMessage = 'è¯­éŸ³è¯†åˆ«å¤±è´¥';
        
        switch (event.error) {
          case 'no-speech':
            errorMessage = 'æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡æ–°å°è¯•';
            break;
          case 'audio-capture':
            errorMessage = 'æ— æ³•è®¿é—®éº¦å…‹é£Žï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®';
            break;
          case 'not-allowed':
            errorMessage = 'éº¦å…‹é£Žæƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸è®¿é—®éº¦å…‹é£Ž';
            break;
          case 'network':
            errorMessage = 'ç½‘ç»œè¿žæŽ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥';
            break;
          case 'aborted':
            errorMessage = 'è¯­éŸ³è¯†åˆ«è¢«ä¸­æ–­';
            break;
          default:
            errorMessage = `è¯­éŸ³è¯†åˆ«é”™è¯¯: ${event.error}`;
        }
        
        setError(errorMessage);
        setIsRecording(false);
        setIsProcessing(false);
      };

      // è¯†åˆ«å¼€å§‹å¤„ç†
      recognition.onstart = () => {
        console.log('ðŸš€ è¯­éŸ³è¯†åˆ«å¼€å§‹');
        setIsRecording(true);
        setIsProcessing(true);
      };

      // å¼€å§‹è¯†åˆ«
      console.log('ðŸŽ¯ å¯åŠ¨è¯­éŸ³è¯†åˆ«...');
      recognition.start();

    } catch (error) {
      console.error('âŒ å¼€å§‹è¯­éŸ³è¯†åˆ«å¤±è´¥:', error);
      setError(error instanceof Error ? error.message : 'æ— æ³•å¯åŠ¨è¯­éŸ³è¯†åˆ«');
      setIsRecording(false);
    }
  }, []);

  const stopRecording = useCallback(() => {
    if (recognitionRef.current && isRecording) {
      console.log('ðŸ›‘ åœæ­¢è¯­éŸ³è¯†åˆ«');
      recognitionRef.current.stop();
      setIsRecording(false);
      setIsProcessing(false);
    }
  }, [isRecording]);

  const clearError = useCallback(() => {
    setError(null);
  }, []);

  const clearTranscript = useCallback(() => {
    setTranscript('');
    setInterimTranscript('');
  }, []);

  return {
    isRecording,
    isProcessing,
    error,
    transcript,
    interimTranscript,
    startRecording,
    stopRecording,
    clearError,
    clearTranscript,
  };
};

export default useVoiceRecognition;